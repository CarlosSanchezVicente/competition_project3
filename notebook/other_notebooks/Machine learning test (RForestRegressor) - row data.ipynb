{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d432db1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931fe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports\n",
    "import time\n",
    "from IPython.display import display, Javascript\n",
    "import itertools\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Data process\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Train\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Models\n",
    "import umap\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Install sqlite as a extension of duckdb\n",
    "#duckdb.install_extension('sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e5bac",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84ff6f",
   "metadata": {},
   "source": [
    "### Functions to correct errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop zeros\n",
    "def drop_zeros(df):\n",
    "    df = df.drop(df[df['x'] == 0].index)\n",
    "    df = df.drop(df[df['y'] == 0].index)\n",
    "    df = df.drop(df[df['z'] == 0].index)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "# Function to remove outliers\n",
    "def remove_outliers(df):\n",
    "    df = df[(df['x'] < 30)]\n",
    "    df = df[(df['y'] < 30)]\n",
    "    df = df[(df['z'] < 7.5) & (df['z'] > 2)]\n",
    "    df = df[(df['table'] < 80) & (df['table'] > 40)]\n",
    "    df = df[(df['depth'] < 75) & (df['depth'] > 45)]\n",
    "    return df\n",
    "\"\"\"\n",
    "\n",
    "def remove_outliers(df):\n",
    "    if 'x' in df.columns:\n",
    "        df = df[df['x'] < 20]\n",
    "    if 'y' in df.columns:\n",
    "        df = df[df['y'] < 20]\n",
    "    if 'z' in df.columns:\n",
    "        df = df[(df['z'] < 7.5) & (df['z'] > 2)]\n",
    "    if 'table' in df.columns:\n",
    "        df = df[(df['table'] < 80) & (df['table'] > 40)]\n",
    "    if 'depth' in df.columns:\n",
    "        df = df[(df['depth'] < 75) & (df['depth'] > 45)]\n",
    "    return df\n",
    "\n",
    "# Function to remove duplicates\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "# Function to impute values:\n",
    "def imputation(df):\n",
    "    # Calculate the median of each column\n",
    "    median_x = df.loc[df['x'] != 0, 'x'].median()\n",
    "    median_y = df.loc[df['y'] != 0, 'y'].median()\n",
    "    median_z = df.loc[df['z'] != 0, 'z'].median()\n",
    "\n",
    "    # Replace values equal to 0 by the corresponding median.\n",
    "    df['x'] = df['x'].replace(0, median_x)\n",
    "    df['y'] = df['y'].replace(0, median_y)\n",
    "    df['z'] = df['z'].replace(0, median_z)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2f8b4",
   "metadata": {},
   "source": [
    "### Functions to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c6751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df):\n",
    "    df_enc = df.copy()\n",
    "\n",
    "    # Obtain the dataframe encoded\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            enc_label = LabelEncoder()\n",
    "            df_enc[column] = enc_label.fit_transform(df[column])\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796d0dd",
   "metadata": {},
   "source": [
    "### Functions to features ingeniering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c5285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ing(df_features):\n",
    "    #print('Dataframe features: ',df_features.head())\n",
    "    # Test the depth calculate\n",
    "    df_features['depth_mm'] = (df_features['z']*2)/(df_features['x'] + df_features['y'])\n",
    "    # Obtain the average girdle diameter\n",
    "    df_features['avg_girdle'] = (df_features['z'])/(df_features['depth_mm'])\n",
    "    # Obtain table in mm\n",
    "    df_features['table_mm'] = (df_features['avg_girdle'])*(df_features['table'])/100\n",
    "    # Obtain table*depth\n",
    "    df_features['table_depth'] = (df_features['table'])/(df_features['depth'])\n",
    "    # Obtain x, y, z\n",
    "    df_features['xyz'] = (df_features['x'])*(df_features['y'])*(df_features['z'])\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332563b",
   "metadata": {},
   "source": [
    "### Functions to remove uncorrelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c65a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete features without correlation with price (train data)\n",
    "def delete_features_train(df):\n",
    "    # Calculate correlation matrix, round with two decimmals\n",
    "    corr_matrix = round(df.corr(numeric_only=True).abs(),2)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    #sns.set (rc = {'figure.figsize':(16, 6)})\n",
    "    #sns.heatmap(corr_matrix, center=0, cmap='BrBG', annot=True)\n",
    "\n",
    "    # Find features with correlation greater than 0.90\n",
    "    to_drop = corr_matrix.columns[corr_matrix['price'] <= 0.1]\n",
    "    #print(to_drop)\n",
    "\n",
    "    # Drop features\n",
    "    df_correct = df\n",
    "    df_correct.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df_correct,to_drop\n",
    "\n",
    "# Function to delete features without correlation with price (test data)\n",
    "def delete_features_test(df, to_drop):\n",
    "    # Calculate correlation matrix, round with two decimmals\n",
    "    corr_matrix = round(df.corr(numeric_only=True).abs(),2)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    #sns.set (rc = {'figure.figsize':(16, 6)})\n",
    "    #sns.heatmap(corr_matrix, center=0, cmap='BrBG', annot=True)\n",
    "\n",
    "    # Drop features\n",
    "    df_correct = df\n",
    "    df_correct.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d4ab6",
   "metadata": {},
   "source": [
    "### Scaling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e8a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stardard_scale_test(df):\n",
    "    X = df.drop('price',axis = 1)\n",
    "    columns = X.columns\n",
    "    # Scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale X\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a691c",
   "metadata": {},
   "source": [
    "### Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b021ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Pipelines\n",
    "\n",
    "#Linear Regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler_1',StandardScaler()),\n",
    "    ('lr_classifier',LinearRegression())\n",
    "])\n",
    "# knn\n",
    "knn_pipline =Pipeline([\n",
    "    ('scaler_2' ,StandardScaler()),\n",
    "    ('knn_classifier',KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "#XGB\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler_3', StandardScaler()),\n",
    "    ('xgb_classifier', XGBRegressor())\n",
    "])\n",
    "\n",
    "\n",
    "#Decision Tree\n",
    "dt_pipeline = Pipeline([\n",
    "    ('scaler_4', StandardScaler()),\n",
    "    ('dt_classifier', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "#Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler_5', StandardScaler()),\n",
    "    ('rf_classifier', RandomForestRegressor(random_state = 42,\n",
    "                                            n_jobs=-1))\n",
    "])\n",
    "\n",
    "#pipelines = [lr_pipeline,knn_pipline,dt_pipeline,rf_pipeline]   #,xgb_pipeline\n",
    "#models = ['Linear Regression', 'KNN', 'Decision Tree', 'Random Forest']   #, 'XGB'\n",
    "\n",
    "pipelines = [rf_pipeline, xgb_pipeline]\n",
    "models = ['Random Forest', 'XGB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c477f5",
   "metadata": {},
   "source": [
    "### Automation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fcd4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version1_without_scaler2(df, drop_zeros_var, imputation_var, remove_outliers_var, remove_duplicates_var,\n",
    "                             feature_ing_var, delete_features_var, encoder_var=1):\n",
    "    # Transformations\n",
    "    if drop_zeros_var == 1:\n",
    "        df = drop_zeros(df)   # Drop zeros\n",
    "        \n",
    "    if imputation_var ==1:\n",
    "        df = imputation(df)   # Imputation data\n",
    "        \n",
    "    if remove_outliers_var == 1:\n",
    "        df = remove_outliers(df)   # Remove outliers\n",
    "        \n",
    "    if remove_duplicates_var == 1:\n",
    "        df = remove_duplicates(df)   # Remove duplicates\n",
    "        \n",
    "    if encoder_var == 1:\n",
    "        df = encoder(df)   # Encoding\n",
    "        \n",
    "    if feature_ing_var == 1:\n",
    "        df = feature_ing(df)   # Feature ingeniering\n",
    "        \n",
    "    if delete_features_var == 1:\n",
    "        df, to_drop = delete_features_train(df)   # Drop features\n",
    "        #print(to_drop)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e7bdc",
   "metadata": {},
   "source": [
    "### Play sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ef18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound():\n",
    "    display(Javascript('new Audio(\"https://www.soundjay.com/button/beep-07.wav\").play()'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd7ce3",
   "metadata": {},
   "source": [
    "## Extraction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8487b014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premium</td>\n",
       "      <td>J</td>\n",
       "      <td>VS2</td>\n",
       "      <td>4268</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>1.21</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS2</td>\n",
       "      <td>505</td>\n",
       "      <td>Kimberly</td>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fair</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>2686</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>0.71</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>738</td>\n",
       "      <td>Kimberly</td>\n",
       "      <td>0.41</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>4882</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>1.02</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cut color clarity  price       city  carat  depth  table     x     y  \\\n",
       "0    Premium     J     VS2   4268      Dubai   1.21   62.4   58.0  6.83  6.79   \n",
       "1  Very Good     H     VS2    505   Kimberly   0.32   63.0   57.0  4.35  4.38   \n",
       "2       Fair     G     VS1   2686  Las Vegas   0.71   65.5   55.0  5.62  5.53   \n",
       "3       Good     D     SI1    738   Kimberly   0.41   63.8   56.0  4.68  4.72   \n",
       "4      Ideal     G     SI1   4882      Dubai   1.02   60.5   59.0  6.55  6.51   \n",
       "\n",
       "      z  \n",
       "0  4.25  \n",
       "1  2.75  \n",
       "2  3.65  \n",
       "3  3.00  \n",
       "4  3.95  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a connection to a file called 'file.db'\n",
    "con = duckdb.connect(\"../data/train/diamonds_train.db\")\n",
    "\n",
    "# Query to extract data from database\n",
    "query_full = \"\"\"\n",
    "SELECT\n",
    "    --tra.index_id,\n",
    "    cut.cut,\n",
    "    col.color,\n",
    "    cla.clarity,\n",
    "    tra.price,\n",
    "    cit.city,\n",
    "    tra.carat,\n",
    "    dim.depth,\n",
    "    dim.table,\n",
    "    dim.x,\n",
    "    dim.y,\n",
    "    dim.z\n",
    "FROM diamonds_properties AS pro\n",
    "JOIN diamonds_cut AS cut ON pro.cut_id = cut.cut_id\n",
    "JOIN diamonds_color AS col ON pro.color_id = col.color_id\n",
    "JOIN diamonds_clarity AS cla ON pro.clarity_id = cla.clarity_id\n",
    "JOIN diamonds_transactional as tra ON pro.index_id = tra.index_id\n",
    "JOIN diamonds_city AS cit ON tra.city_id = cit.city_id\n",
    "JOIN diamonds_dimensions AS dim ON pro.index_id = dim.index_id\n",
    "\"\"\"\n",
    "\n",
    "diamond_train_df = con.execute(query_full).df()\n",
    "diamond_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "810dc93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.18</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.57</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.32</td>\n",
       "      <td>4.57</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.90</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  carat        cut color clarity  depth  table     x     y     z  \\\n",
       "0   0   0.79  Very Good     F     SI1   62.7   60.0  5.82  5.89  3.67   \n",
       "1   1   1.20      Ideal     J     VS1   61.0   57.0  6.81  6.89  4.18   \n",
       "2   2   1.57    Premium     H     SI1   62.2   61.0  7.38  7.32  4.57   \n",
       "3   3   0.90  Very Good     F     SI1   63.8   54.0  6.09  6.13  3.90   \n",
       "4   4   0.50  Very Good     F     VS1   62.9   58.0  5.05  5.09  3.19   \n",
       "\n",
       "        city  \n",
       "0  Amsterdam  \n",
       "1      Surat  \n",
       "2   Kimberly  \n",
       "3   Kimberly  \n",
       "4  Amsterdam  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamond_test_df = pd.read_csv(\"../data/test/diamonds_test.csv\")\n",
    "diamond_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6306bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ffb11",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5db5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Definir las columnas con los nombres deseados\n",
    "column_names = ['model', 'cv_score', 'prediction', 'drop_zeros_var', 'imputation_var', 'remove_outliers_var', \n",
    "                'remove_duplicates_var', 'encoder_var', 'feature_ing_var', 'delete_features_var', 'cut', 'color', 'clarity', \n",
    "                'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'depth_mm', 'avg_girdle', 'table_mm', 'table_depth', 'xyz']\n",
    "\n",
    "# Asignar las columnas al DataFrame\n",
    "result_df = result_df.reindex(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6af6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_features_choice(df, pipelines, models, result_df):\n",
    "    # Combinations\n",
    "    cut         = [1]\n",
    "    color       = [1]   # [0, 1]\n",
    "    clarity     = [1]\n",
    "    city        = [1]\n",
    "    carat       = [1]\n",
    "    depth       = [1]\n",
    "    table       = [1]\n",
    "    x           = [1]\n",
    "    y           = [1]\n",
    "    z           = [1]\n",
    "    depth_mm    = [0]\n",
    "    avg_girdle  = [0]\n",
    "    table_mm    = [0]\n",
    "    table_depth = [0]\n",
    "    xyz         = [0]\n",
    "    \n",
    "\n",
    "    # Generar todas las combinaciones posibles\n",
    "    combinations_features = itertools.product(cut, color, clarity, city, carat, depth, table, x, y, z, depth_mm, \n",
    "                                              avg_girdle, table_mm, table_depth, xyz)\n",
    "\n",
    "    #results = []\n",
    "    for comb in combinations_features:\n",
    "        #start_time = time.time()\n",
    "        \n",
    "        # Transform train data\n",
    "        df_transform = version1_without_scaler2(df,\n",
    "                                                drop_zeros_var=1,\n",
    "                                                imputation_var=0,\n",
    "                                                remove_outliers_var=1,\n",
    "                                                remove_duplicates_var=1,\n",
    "                                                feature_ing_var=1,\n",
    "                                                delete_features_var=0)\n",
    "        \n",
    "        # Obtain the features to train the model\n",
    "        cut, color, clarity, city, carat, depth, table, x, y, z, depth_mm,\\\n",
    "            avg_girdle, table_mm, table_depth, xyz = comb\n",
    "        \n",
    "        # List of all possible feature names\n",
    "        features_to_train = []\n",
    "        all_features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'depth_mm', \n",
    "                       'avg_girdle', 'table_mm', 'table_depth', 'xyz']\n",
    "        \n",
    "        # Iterate over features and add them to variables_with_value_1 if they have a value of 1\n",
    "        for feature in all_features:\n",
    "            if locals()[feature] == 1:\n",
    "                features_to_train.append(feature)\n",
    "\n",
    "        # splitting the dataset in test and train data . The prece will be the Target and the other columns the features\n",
    "        X = df_transform[features_to_train]\n",
    "        y = df_transform['price']\n",
    "        \n",
    "        # There are combinations in which all values are zero, in that case it isn't necessary train model\n",
    "        if df_transform.shape[1] >= 1:\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "        \n",
    "            # Fit our models to the training data\n",
    "            for i in pipelines :\n",
    "                i.fit(X_train , y_train)\n",
    "\n",
    "            cv_results = []\n",
    "            for i, model in enumerate(pipelines):\n",
    "                # Cross validation\n",
    "                cv_score = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "                cv_results.append(cv_score)\n",
    "\n",
    "                # Test the result\n",
    "                pred = model.predict(X_test)\n",
    "\n",
    "                # Create list with featers used\n",
    "                features_value_list = list(comb)\n",
    "                \n",
    "                # Create list with process parameters\n",
    "                cv_score_mean = abs(np.mean(cv_results))\n",
    "                rmse = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "                process_list = [model, cv_score_mean, rmse, 1, 0, 1, 1, 1, 1, 0]\n",
    "                \n",
    "                # Create complete list\n",
    "                registros = process_list + features_value_list\n",
    "                \n",
    "                # Store the results in the results dataframe\n",
    "                result_df.loc[len(result_df.index)] = registros\n",
    "                print(model, rmse, comb)\n",
    "                \n",
    "            #end_time = time.time()\n",
    "            #print(\"Execution time: \", end_time - start_time)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelines = [rf_pipeline]   #[lr_pipeline, knn_pipline, dt_pipeline, rf_pipeline, xgb_pipeline]\n",
    "models = ['Random Forest']   #['Linear Regression', 'KNN', 'Decision Tree', 'Random Forest', 'XGB']\n",
    "\n",
    "# Execute functions and sort data\n",
    "result_df = automate_features_choice(diamond_train_df, pipelines, models, result_df)\n",
    "#play_sound()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "result_df_sorted = result_df.sort_values(by='prediction')\n",
    "result_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5c9d3",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the non-linear relation between features\n",
    "X = transformed_df.copy()\n",
    "y = X.pop(\"price\")\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index = X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8540b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate log\n",
    "def calculate_log(df, name):\n",
    "    list_log= []\n",
    "    for i in df['carat']:\n",
    "        list_log.append(math.log(i))\n",
    "    \n",
    "    new_name = name + '_log'\n",
    "    df[new_name] = list_log\n",
    "    return df\n",
    "\n",
    "# Funtion to classify diamond shape\n",
    "def classify_shape(df):\n",
    "    shape = []\n",
    "    for i in df['table'].index:\n",
    "        if 54<df['table'][i]<57 and 61<df['depth'][i]<62.5:\n",
    "            shape.append('Round')\n",
    "        elif 52<df['table'][i]<60 and 60<df['depth'][i]<68:\n",
    "            shape.append('Oval')\n",
    "        elif 63<df['table'][i]<69 and 69<df['depth'][i]<76:\n",
    "            shape.append('Princess')\n",
    "        elif 58<df['table'][i]<63 and 58<df['depth'][i]<66:\n",
    "            shape.append('Cushion')\n",
    "        else:\n",
    "            shape.append('others')\n",
    "            \n",
    "    df['shape'] = shape\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffa425",
   "metadata": {},
   "source": [
    "## Transform and train diamond_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d4143fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>price</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.21</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.71</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2686</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.41</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "      <td>738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.02</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cut  color  clarity  city  carat  depth  table     x     y     z  price  \\\n",
       "0    3      6        5     2   1.21   62.4   58.0  6.83  6.79  4.25   4268   \n",
       "1    4      4        5     3   0.32   63.0   57.0  4.35  4.38  2.75    505   \n",
       "2    0      3        4     4   0.71   65.5   55.0  5.62  5.53  3.65   2686   \n",
       "3    1      0        2     3   0.41   63.8   56.0  4.68  4.72  3.00    738   \n",
       "4    2      3        2     2   1.02   60.5   59.0  6.55  6.51  3.95   4882   \n",
       "\n",
       "   shape  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform\n",
    "transformed_df = classify_shape(diamond_train_df)\n",
    "transformed_df = encoder(transformed_df)\n",
    "transformed_df = drop_zeros(transformed_df)\n",
    "#transformed_df = imputation(transformed_df)\n",
    "transformed_df = remove_outliers(transformed_df)\n",
    "transformed_df = remove_duplicates(transformed_df)\n",
    "transformed_df = feature_ing(transformed_df)\n",
    "\n",
    "# default = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'price']\n",
    "\n",
    "all_features = ['cut', 'color', 'clarity', 'city', 'depth','carat', 'depth', 'table', 'x', 'y', 'z', 'depth_mm', \n",
    "                'avg_girdle', 'table_mm', 'table_depth', 'xyz', 'price', 'carat_log', 'x_log', 'y_log',\n",
    "                'z_log', 'ratio_length_width', 'ratio_length_width_depth', 'volume', 'density']\n",
    "selection_features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'price', 'shape']\n",
    "\n",
    "transformed_df = calculate_log(transformed_df, 'carat')\n",
    "transformed_df = calculate_log(transformed_df, 'x')\n",
    "transformed_df = calculate_log(transformed_df, 'y')\n",
    "transformed_df = calculate_log(transformed_df, 'z')\n",
    "\n",
    "transformed_df['ratio_length_width'] = transformed_df['x']/transformed_df['y']\n",
    "transformed_df['ratio_length_width_depth'] = transformed_df['x']/transformed_df['y']/transformed_df['z']\n",
    "transformed_df['volume'] = transformed_df['x']*transformed_df['y']*transformed_df['z']\n",
    "transformed_df['density'] = transformed_df['carat']/transformed_df['volume']\n",
    "\n",
    "#Doesn't make sense to have diamonds with width and depth higher than 20:\n",
    "#transformed_df = transformed_df.loc[~(transformed_df['carat_log'] == 0 )]\n",
    "\n",
    "transformed_df_2 = transformed_df[selection_features]\n",
    "transformed_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6263952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 50, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  | cv_score_mean: 575.92611795982  | rmse: 558.2782585570247\n",
      "CPU times: total: 44.2 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "# Transform\n",
    "transformed_df = encoder(diamond_train_df)\n",
    "transformed_df = drop_zeros(transformed_df)\n",
    "transformed_df = remove_outliers(transformed_df)\n",
    "transformed_df = remove_duplicates(transformed_df)\n",
    "transformed_df = feature_ing(transformed_df)\n",
    "#scaled_df = stardard_scale_test(transformed_df)\n",
    "\n",
    "X = transformed_df.drop('price',axis = 1)\n",
    "y = transformed_df['price']\n",
    "\"\"\"\n",
    "X = transformed_df_2.drop('price',axis = 1)\n",
    "y = transformed_df_2['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "\n",
    "# Model\n",
    "\n",
    "model = RandomForestRegressor(random_state=42,\n",
    "                              n_estimators=50,\n",
    "                              max_depth=None,\n",
    "                              max_features=None,\n",
    "                              min_samples_leaf=1,\n",
    "                              min_samples_split=2,\n",
    "                              n_jobs=-1)\n",
    "\"\"\"\n",
    "model = GradientBoostingRegressor(random_state = 42,\n",
    "                                  n_estimators=100)\n",
    "\"\"\"\n",
    "\n",
    "# Cross validation\n",
    "cv_results = []\n",
    "cv_score = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "cv_results.append(cv_score)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "# Prints\n",
    "hyperparameters = model.get_params()\n",
    "cv_score_mean = abs(np.mean(cv_results))\n",
    "rmse = mean_squared_error(y_test, y_pred)**0.5\n",
    "\n",
    "print('Hyperparameters: ', hyperparameters, ' | cv_score_mean:', cv_score_mean, ' | rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32b077bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimaze = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4963c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features = ['cut', 'color', 'clarity', 'city', 'depth', 'table', 'carat_log', 'x_log', 'y_log', 'z_log',\n",
    "#             'ratio_length_width', 'ratio_length_width_depth', 'volume', 'density', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "# Submission = 597\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 563.6224504060208  | rmse: 541.8872683394235\n",
    "CPU times: total: 17min 7s\n",
    "Wall time: 2min 22s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'depth', 'table', 'carat_log', \n",
    "#             'ratio_length_width', 'ratio_length_width_depth', 'volume', 'density', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500\n",
    "# Submission = 597\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 563.1380099851094  | rmse: 541.9037907071685\n",
    "CPU times: total: 14min 32s\n",
    "Wall time: 1min 56s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'depth', 'table','x', 'y', 'z', 'carat_log', \n",
    "#             'ratio_length_width', 'ratio_length_width_depth', 'volume', 'density', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 568.2830813092132  | rmse: 545.7160325766089\n",
    "CPU times: total: 18min 18s\n",
    "Wall time: 2min 32s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'carat_log', 'depth', 'table', 'price']\n",
    "# encoding, imputation, remove_outliers y remove_duplicates | n_stimators=500\n",
    "# Submission = 552\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 578.5952143324417  | rmse: 552.891156245962\n",
    "CPU times: total: 4min 37s\n",
    "Wall time: 40.2 s\n",
    "\n",
    "# Features de serie, encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500\n",
    "# Submission = 543---------------------------------------------------------------\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.0212938830194  | rmse: 552.9419513838835  | rmse_2: 552.9419513838835\n",
    "CPU times: total: 9min 28s\n",
    "Wall time: 1min 22s\n",
    "    \n",
    "# Features de serie, encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=600\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 600, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 569.6888950649824  | rmse: 553.1880851479532  | rmse_2: 553.1880851479532\n",
    "CPU times: total: 11min 53s\n",
    "Wall time: 1min 40s\n",
    "\n",
    "# Features de serie, encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=400\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 300, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.392820611711  | rmse: 553.2600932696147  | rmse_2: 553.2600932696147\n",
    "CPU times: total: 4min 35s\n",
    "Wall time: 42.8 s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'depth', 'table', 'x', 'y', 'z', 'carat_log', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.336771427553  | rmse: 553.3567161129744\n",
    "CPU times: total: 9min 54s\n",
    "Wall time: 1min 32s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'depth', 'table', 'x', 'y', 'z', 'carat_log', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.2595671275265  | rmse: 553.4054287603929\n",
    "CPU times: total: 9min 10s\n",
    "Wall time: 1min 14s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'depth', 'table', 'x', 'y', 'z', 'carat_log', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=1500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 1500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 569.5750376223212  | rmse: 553.6604440230118\n",
    "CPU times: total: 31min 48s\n",
    "Wall time: 4min 27s\n",
    "    \n",
    "# Features de serie, encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=400\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 573.2463024252991  | rmse: 553.7944242193641  | rmse_2: 553.7944242193641\n",
    "CPU times: total: 1min 27s\n",
    "Wall time: 13.4 s\n",
    "    \n",
    "# Features de serie, encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=400\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 574.2712643507491  | rmse: 554.488321315124  | rmse_2: 554.488321315124\n",
    "CPU times: total: 1min 23s\n",
    "Wall time: 13.1 s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'price', 'shape']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500\n",
    "# Submission = 542 ----------------------------------------------------------\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.1513571076711  | rmse: 554.7242488195719\n",
    "CPU times: total: 10min 2s\n",
    "Wall time: 1min 25s\n",
    "    \n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'price', 'shape']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500\n",
    "# Submission = 542 ----------------------------------------------------------\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.1513571076711  | rmse: 554.7242488195719\n",
    "CPU times: total: 9min 52s\n",
    "Wall time: 1min 20s\n",
    "    \n",
    "# Features de serie, encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=400\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 400, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 568.2230549678686  | rmse: 555.2393808873512  | rmse_2: 555.2393808873512\n",
    "CPU times: total: 15min 42s\n",
    "Wall time: 2min 12s\n",
    "    \n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'table','x', 'y', 'z', 'carat_log', 'price'] \n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 571.09845582581  | rmse: 555.2875203635209  | rmse_2: 555.2875203635209\n",
    "CPU times: total: 8min 40s\n",
    "Wall time: 1min 11s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat_log', 'table','x', 'y', 'z', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=50\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.9756633240581  | rmse: 555.6053281565353  | rmse_2: 555.6053281565353\n",
    "CPU times: total: 8min 43s\n",
    "Wall time: 1min 15s\n",
    "    \n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'x', 'y', 'z','avg_girdle', 'table_mm', 'xyz', 'price'] \n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500   \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 567.5032514631432  | rmse: 555.7750839709606  | rmse_2: 555.7750839709606\n",
    "CPU times: total: 14min 5s\n",
    "Wall time: 1min 53s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'carat_log', 'table','x', 'y', 'z', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=50\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 569.945980050026  | rmse: 556.4799516795906  | rmse_2: 556.4799516795906\n",
    "CPU times: total: 6min 35s\n",
    "Wall time: 55 s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'x', 'y', 'z', 'table_mm', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=50\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 570.4899544500876  | rmse: 556.0081586081567  | rmse_2: 556.0081586081567\n",
    "CPU times: total: 10min 40s\n",
    "Wall time: 1min 30s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'x', 'y', 'z', 'avg_girdle', 'table_mm', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=50\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 569.6400678426654  | rmse: 556.1522654109002  | rmse_2: 556.1522654109002\n",
    "CPU times: total: 12min 2s\n",
    "Wall time: 1min 44s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'price', 'shape']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=50\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 50, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 575.92611795982  | rmse: 558.2782585570247\n",
    "CPU times: total: 44.2 s\n",
    "Wall time: 7.55 s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'price', 'shape']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=1500\n",
    "# Submision = 541 ---------------------------------------------------------------\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 1500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 575.92611795982  | rmse: 558.2782585570247\n",
    "CPU times: total: 44.2 s\n",
    "Wall time: 7.55 s\n",
    "\n",
    "# Features = [['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'avg_girdle', 'table_mm', 'xyz', 'price']] \n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 571.0132381609183  | rmse: 562.8086785378399  | rmse_2: 562.8086785378399\n",
    "CPU times: total: 10min 44s\n",
    "Wall time: 1min 26s\n",
    "\n",
    "# Features = ['cut', 'color', 'clarity', 'city', 'carat_log', 'depth', 'table','x_log', 'y_log', 'z_log', 'price']\n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500 \n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 578.4665781312609  | rmse: 563.1005194121811\n",
    "CPU times: total: 7min 48s\n",
    "Wall time: 1min 7s\n",
    "    \n",
    "# Features de serie y solo encoding | n_stimators=400\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 568.7680257934356  | rmse: 587.9591159724442  | rmse_2: 587.9591159724442\n",
    "CPU times: total: 1min 2s\n",
    "Wall time: 1min 3s\n",
    "    \n",
    "# Features = ['color', 'carat', 'table', 'x', 'y', 'z', 'price'] \n",
    "# encoding, drop_zeros, remove_outliers y remove_duplicates | n_stimators=500\n",
    "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  \n",
    "    | cv_score_mean: 1193.7263388731408  | rmse: 1176.9075341911703  | rmse_2: 1176.9075341911703\n",
    "CPU times: total: 6min 9s\n",
    "Wall time: 53.4 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ab2c6",
   "metadata": {},
   "source": [
    "## Transform test data and obtain the prediction to upload in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "291c7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "def transformation_data(df, type_data):\n",
    "    trans_df = classify_shape(df)\n",
    "    trans_df = encoder(trans_df)\n",
    "    trans_df = imputation(trans_df)\n",
    "\n",
    "    if type_data == 'train_data':\n",
    "        #trans_df = drop_zeros(trans_df)\n",
    "        trans_df = remove_outliers(trans_df)\n",
    "        trans_df = remove_duplicates(trans_df)\n",
    "        selection_features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'shape', 'price']\n",
    "        \n",
    "        #selection_features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table','x', 'y', 'z', 'price']\n",
    "    if type_data == 'test_data':\n",
    "        selection_features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'shape']\n",
    "        #selection_features = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "    trans_df = feature_ing(trans_df)\n",
    "\n",
    "    trans_df = calculate_log(trans_df, 'carat')\n",
    "    trans_df = calculate_log(trans_df, 'x')\n",
    "    trans_df = calculate_log(trans_df, 'y')\n",
    "    trans_df = calculate_log(trans_df, 'z')\n",
    "\n",
    "\n",
    "    trans_df['ratio_length_width'] = trans_df['x']/trans_df['y']\n",
    "    trans_df['ratio_length_width_depth'] = trans_df['x']/trans_df['y']/trans_df['z']\n",
    "    trans_df['volume'] = trans_df['x']*trans_df['y']*trans_df['z']\n",
    "    trans_df['density'] = trans_df['carat']/trans_df['volume']\n",
    "    \n",
    "    trans_df_2 = trans_df[selection_features]\n",
    "    trans_df_2.head()\n",
    "    \n",
    "    return trans_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09b49101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 100, 'max_features': None, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 1500, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  | cv_score_mean: 556.6704984876692\n",
      "CPU times: total: 1h 17s\n",
      "Wall time: 8min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Transform diamond_train_df\n",
    "train_df = transformation_data(diamond_train_df, 'train_data')\n",
    "X_train = train_df.drop('price',axis = 1)\n",
    "y_train = train_df['price']\n",
    "\n",
    "# Transform diamond_test_df\n",
    "X_test = transformation_data(diamond_test_df, 'test_data')\n",
    "\n",
    "# Model\n",
    "model = RandomForestRegressor(random_state=42,\n",
    "                              n_estimators=1500,\n",
    "                              max_depth=100,\n",
    "                              max_features=None,\n",
    "                              min_samples_leaf=1,\n",
    "                              min_samples_split=5,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "# Cross validation\n",
    "cv_results = []\n",
    "cv_score = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "cv_results.append(cv_score)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "# Prints\n",
    "hyperparameters = model.get_params()\n",
    "cv_score_mean = abs(np.mean(cv_results))\n",
    "#rmse = mean_squared_error(y_test, y_pred)**0.5\n",
    "\n",
    "print('Hyperparameters: ', hyperparameters, ' | cv_score_mean:', cv_score_mean)   #, ' | rmse:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb5f2a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13485"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e1deddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.reset_index(inplace=True)\n",
    "y_pred_df.columns = ['id', 'price']\n",
    "y_pred_df.to_csv('submission_1500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e2de1",
   "metadata": {},
   "source": [
    "# Optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfad7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "\n",
    "\n",
    "param_grid = {'n_estimators': [450, 500],  # Number of trees in the forest.\n",
    "              'max_depth': [None, 3, 10],  # Maximum depth of the trees.\n",
    "              'min_samples_split': [2, 10],  # Minimum number of samples required to split an internal node.\n",
    "              'min_samples_leaf': [1, 4],  # Minimum number of samples required to be at a leaf node.\n",
    "              'max_features': [None, 'sqrt', 'log2']  # Number of features to consider when looking for the best split.\n",
    "              }\n",
    "\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=3,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c81b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3_env",
   "language": "python",
   "name": "m3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
