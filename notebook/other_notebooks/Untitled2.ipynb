{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae75d81",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26001bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports\n",
    "import itertools\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Install sqlite as a extension of duckdb\n",
    "#duckdb.install_extension('sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f18c4",
   "metadata": {},
   "source": [
    "## Imports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514c50e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'duckdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create a connection to a file called 'file.db'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m con \u001b[38;5;241m=\u001b[39m \u001b[43mduckdb\u001b[49m\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/train/diamonds_train.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Query to extract data from database\u001b[39;00m\n\u001b[0;32m      5\u001b[0m query_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    --tra.index_id,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124mJOIN diamonds_dimensions AS dim ON pro.index_id = dim.index_id\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'duckdb' is not defined"
     ]
    }
   ],
   "source": [
    "# create a connection to a file called 'file.db'\n",
    "con = duckdb.connect(\"../data/train/diamonds_train.db\")\n",
    "\n",
    "# Query to extract data from database\n",
    "query_full = \"\"\"\n",
    "SELECT\n",
    "    --tra.index_id,\n",
    "    cut.cut,\n",
    "    col.color,\n",
    "    cla.clarity,\n",
    "    tra.price,\n",
    "    cit.city,\n",
    "    tra.carat,\n",
    "    dim.depth,\n",
    "    dim.table,\n",
    "    dim.x,\n",
    "    dim.y,\n",
    "    dim.z\n",
    "FROM diamonds_properties AS pro\n",
    "JOIN diamonds_cut AS cut ON pro.cut_id = cut.cut_id\n",
    "JOIN diamonds_color AS col ON pro.color_id = col.color_id\n",
    "JOIN diamonds_clarity AS cla ON pro.clarity_id = cla.clarity_id\n",
    "JOIN diamonds_transactional as tra ON pro.index_id = tra.index_id\n",
    "JOIN diamonds_city AS cit ON tra.city_id = cit.city_id\n",
    "JOIN diamonds_dimensions AS dim ON pro.index_id = dim.index_id\n",
    "\"\"\"\n",
    "\n",
    "df_train = con.execute(query_full).df()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e7371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.79</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.18</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.57</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.32</td>\n",
       "      <td>4.57</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.90</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat        cut color clarity  depth  table     x     y     z       city\n",
       "0   0.79  Very Good     F     SI1   62.7   60.0  5.82  5.89  3.67  Amsterdam\n",
       "1   1.20      Ideal     J     VS1   61.0   57.0  6.81  6.89  4.18      Surat\n",
       "2   1.57    Premium     H     SI1   62.2   61.0  7.38  7.32  4.57   Kimberly\n",
       "3   0.90  Very Good     F     SI1   63.8   54.0  6.09  6.13  3.90   Kimberly\n",
       "4   0.50  Very Good     F     VS1   62.9   58.0  5.05  5.09  3.19  Amsterdam"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data/test/diamonds_test.csv\")\n",
    "df_test.drop(columns='id', inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647546cb",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "753b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type_trans = 'train' or 'test'\n",
    "\n",
    "def transformations(df, type_trans):\n",
    "    if type_trans == 'train':\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    #df.rename(columns={'x': 'length', 'y': 'width', 'depth': 'depth','table':'table_width'}, inplace=True)\n",
    "    \n",
    "    # Encode\n",
    "    cut_categories = df['cut'].unique()\n",
    "    color_categories = df['color'].unique()\n",
    "    clarity_categories = df['clarity'].unique()\n",
    "    city_categories = df['city'].unique()\n",
    "    \n",
    "    # Change to categorical\n",
    "    df['cut'] = pd.Categorical(df['cut'], categories= cut_categories, ordered=True)\n",
    "    df['color'] = pd.Categorical(df['color'], categories= color_categories, ordered=True)\n",
    "    df['clarity'] = pd.Categorical(df['clarity'], categories= clarity_categories, ordered=True)\n",
    "    df['city'] = pd.Categorical(df['city'], categories= city_categories, ordered=True)\n",
    "    \n",
    "    cat_cols = ['cut','color','clarity', 'city']\n",
    "    cat_orders = [cut_categories, color_categories, clarity_categories, city_categories]\n",
    "    encoder = OrdinalEncoder(categories=cat_orders)\n",
    "    cats_encoded = pd.DataFrame(encoder.fit_transform(df[cat_cols]), columns = ['cut_encoded','color_encoded',\n",
    "                                                                                'clarity_encoded','city_encoded'])\n",
    "    \n",
    "    # Store encoded columns\n",
    "    df_encoded = df.drop(columns=['cut','color','clarity','city']).copy()\n",
    "    df_encoded['cut_encoded'] = df['cut'].cat.codes\n",
    "    df_encoded['color_encoded'] = df['color'].cat.codes\n",
    "    df_encoded['clarity_encoded'] = df['clarity'].cat.codes\n",
    "    df_encoded['city_encoded'] = df['city'].cat.codes\n",
    "    \n",
    "    # Calculate volume\n",
    "    df['volume'] = df['z'] * df['y'] * df['z']\n",
    "    df_encoded['volume'] = df['volume']\n",
    "    \n",
    "    clean_df = df_encoded.copy()\n",
    "    if type_trans == 'train':\n",
    "        outliers_cols = ['x','y','z','table','depth','volume']\n",
    "\n",
    "        # setting values that above or lower than the whiskers in the box plot to NaNs\n",
    "        for col in outliers_cols:\n",
    "\n",
    "            data = clean_df[col]\n",
    "\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            min = Q1 - (1.5 * IQR)\n",
    "            max = Q3 + (1.5 * IQR)\n",
    "\n",
    "            outliers = ( (data < min) | (data > max) )\n",
    "\n",
    "            clean_df.loc[outliers, col] = np.nan\n",
    "\n",
    "        clean_df.isna().sum()\n",
    "    \n",
    "    # Remove NaN\n",
    "    imputer = IterativeImputer(max_iter=50)\n",
    "    clean_df = pd.DataFrame(imputer.fit_transform(clean_df), columns=clean_df.columns, index=clean_df.index)\n",
    "    \n",
    "    # Change cut to cut_encoded...\n",
    "    df_all = clean_df.copy()\n",
    "    df_all[['cut','color','clarity','city']] = df[['cut','color','clarity','city']] \n",
    "    df = df_all.drop(columns=['cut_encoded','color_encoded','clarity_encoded','city_encoded'])\n",
    "    \n",
    "    # Transform dataframe\n",
    "    Skewed_Cols = clean_df[['carat','volume']].columns\n",
    "    trans_df = clean_df.copy()\n",
    "    for col in Skewed_Cols:\n",
    "        trans_df[col] = np.log(1 + trans_df[col])\n",
    "    \n",
    "    # POLYNOMICAL FEATURES\n",
    "    if type_trans == 'train':\n",
    "        price_column = trans_df['price']\n",
    "        df_ploy = trans_df.drop(columns=['price']).copy()\n",
    "    else:\n",
    "        df_ploy = trans_df\n",
    "        \n",
    "    poly = PolynomialFeatures(2)\n",
    "    ploy_data = poly.fit_transform(df_ploy)\n",
    "    df_ploy = pd.DataFrame(ploy_data, columns=poly.get_feature_names_out())\n",
    "    # Scale dataframe with polynomical\n",
    "    df_scaled = df_ploy.copy()\n",
    "    Scaler = StandardScaler()\n",
    "    scaled_data = Scaler.fit_transform(df_scaled)\n",
    "    df_scaled = pd.DataFrame(scaled_data, columns=df_scaled.columns)\n",
    "    \n",
    "    # WITHOUT POLYNOMICAL FEATURES\n",
    "    if type_trans == 'train':\n",
    "        price_column = trans_df['price']\n",
    "        org_df_scaled = trans_df.drop(columns=['price']).copy()\n",
    "    else:\n",
    "        org_df_scaled = trans_df\n",
    "        \n",
    "    Scaler = StandardScaler()\n",
    "    scaled_data = Scaler.fit_transform(org_df_scaled)\n",
    "    org_df_scaled = pd.DataFrame(scaled_data, columns=org_df_scaled.columns)\n",
    "    \n",
    "    if type_trans == 'train':\n",
    "        return df_scaled, org_df_scaled, price_column \n",
    "    else:\n",
    "        return df_scaled, org_df_scaled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39db31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type_trans = 'train' or 'test'\n",
    "df_train_trans, df_train_trans_no_ploy, price_column = transformations(df_train, 'train')\n",
    "df_test_trans, df_test_trans_no_ploy = transformations(df_test, 'test') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af82ba5",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc5d1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_trans_no_ploy\n",
    "y = price_column\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "141a961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "-551.979028568908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.75,\n",
       " 'gamma': 0.12,\n",
       " 'lambda': 0.12,\n",
       " 'learning_rate': 0.02,\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 1300,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#param_Rodri = {'colsample_bytree': 0.95, 'gamma': 0.14, 'learning_rate': 0.012, 'max_depth': 7, 'missing': np.inf, \n",
    "#               'n_estimators': 1130, 'subsample': 0.8}\n",
    "\n",
    "param_grid = {'n_estimators': [1300],  # Number of trees in the forest.\n",
    "              'max_depth': [5],  # Maximum depth of the trees.\n",
    "              'subsample': [1],\n",
    "              'colsample_bytree': [0.75],\n",
    "              'lambda': [0.12],\n",
    "              'gamma': [0.12],\n",
    "              'learning_rate': [0.02]\n",
    "              }\n",
    "\n",
    "xgb_reg = XGBRegressor(random_state=0)\n",
    "\n",
    "xgb_grid_search = GridSearchCV(xgb_reg, param_grid, cv=3, scoring='neg_root_mean_squared_error', return_train_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "xgb_grid_search.fit(X, y)\n",
    "\n",
    "print('\\n')\n",
    "print('Best hyperparameters: ', xgb_grid_search.best_params_, '\\n')\n",
    "print('Best score: ', xgb_grid_search.best_score_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b14b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1803\n",
      "[LightGBM] [Info] Number of data points in the train set: 40445, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 3928.215923\n",
      "\n",
      "\n",
      "Best hyperparameters:  {'learning_rate': 0.01, 'n_estimators': 2650, 'num_leaves': 20} \n",
      "\n",
      "Best score:  -544.9910524913057 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    " {'num_leaves': [20], 'learning_rate' : [0.01] ,\n",
    "  'n_estimators' : [2650]},]\n",
    "\n",
    "lgbm_reg = LGBMRegressor(random_state=0)\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(lgbm_reg, param_grid, cv=3, scoring='neg_root_mean_squared_error', return_train_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "lgbm_grid_search.fit(X, y)\n",
    "\n",
    "print('\\n')\n",
    "print('Best hyperparameters: ', lgbm_grid_search.best_params_, '\\n')\n",
    "print('Best score: ', lgbm_grid_search.best_score_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5f0963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "\n",
      "Best hyperparameters:  {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 500} \n",
      "\n",
      "Best score:  -696.7412070404639 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{ 'n_estimators': [500], 'max_depth' : [None],\n",
    "                'max_features' : ['sqrt']}]\n",
    "\n",
    "extrees_reg = ExtraTreesRegressor(random_state=0)\n",
    "\n",
    "extrees_grid_search = GridSearchCV(extrees_reg, param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "extrees_grid_search.fit(X, y)\n",
    "\n",
    "print('\\n')\n",
    "print('Best hyperparameters: ', extrees_grid_search.best_params_, '\\n')\n",
    "print('Best score: ', extrees_grid_search.best_score_, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f8fe",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a97d9",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b34c74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBRegressor\n",
    "xgb_param_grid = {'n_estimators': 1300,  # Number of trees in the forest.\n",
    "              'max_depth': 5,  # Maximum depth of the trees.\n",
    "              'subsample': 1,\n",
    "              'colsample_bytree': 0.75,\n",
    "              'lambda': 0.12,\n",
    "              'gamma': 0.12,\n",
    "              'learning_rate': 0.02,\n",
    "              'random_state':0\n",
    "              }\n",
    "xgb_model = XGBRegressor(**xgb_param_grid)\n",
    "#xgb_model.fit(X_train,y_train)\n",
    "\n",
    "# LGBMRegresor\n",
    "lgb_param_grid = {'num_leaves': 20, \n",
    "                  'learning_rate' : 0.01,\n",
    "                  'n_estimators' : 2650,\n",
    "                  'random_state': 0\n",
    "                 }\n",
    "#lgb_model = LGBMRegressor(num_leaves=20, learning_rate=0.01,n_estimators=2650,random_state': 0)\n",
    "lgb_model = LGBMRegressor(**lgb_param_grid)\n",
    "#lgb_model.fit(X_train,y_train)\n",
    "\n",
    "# Extrees\n",
    "extrees_param_grid = {'n_estimators': 500, \n",
    "                      'max_depth' : None,\n",
    "                      'max_features' : 'sqrt',\n",
    "                      'random_state':0\n",
    "                     }\n",
    "#extrees_model = ExtraTreesRegressor(n_estimators=500, max_depth=None,max_features='sqrt',random_state=0)\n",
    "extrees_model = ExtraTreesRegressor(**extrees_param_grid)\n",
    "#extrees_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac08897",
   "metadata": {},
   "source": [
    "### Train model - stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6875975",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:14\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators = [('lgbm1', xgb_model),\n",
    "              ('xgb1', lgb_model),\n",
    "              ('extrees', extrees_model),]\n",
    "\n",
    "final_estimator = XGBRegressor()  # Definir el estimador final\n",
    "\n",
    "stack_model = StackingRegressor(estimators=estimators, #final_estimator=final_estimator,\n",
    "                                cv=None, n_jobs=-1, verbose=True, passthrough=True)\n",
    "\n",
    "cv_results = []\n",
    "cv_score = cross_val_score(stack_model, X, y, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "cv_results.append(cv_score)\n",
    "\n",
    "hyperparameters = model.get_params()\n",
    "cv_score_mean = abs(np.mean(cv_results))\n",
    "print('Hyperparameters: ', hyperparameters, ' | cv_score_mean:', cv_score_mean)\n",
    "\n",
    "stack_model.fit(X,y)\n",
    "hyperparameters_all = stack_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26c4e4",
   "metadata": {},
   "source": [
    "### Train model - RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c74c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4603689",
   "metadata": {},
   "source": [
    "### Obtain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ded7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_all.predict(df_test_trans_no_ploy)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeac3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store the dataframe to upload to kaggle\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.reset_index(inplace=True)\n",
    "y_pred_df.columns = ['id', 'price']\n",
    "y_pred_df.to_csv('../data/submisions/XGB_all_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3_env",
   "language": "python",
   "name": "m3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
