{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e319a3",
   "metadata": {},
   "source": [
    "# PIPELINE AND PRODUCTION CODE\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15d64f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports\n",
    "import itertools\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import umap\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Install sqlite as a extension of duckdb\n",
    "#duckdb.install_extension('sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e6e53",
   "metadata": {},
   "source": [
    "# Functions\n",
    "### Functions to correct errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ee2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop zeros\n",
    "def drop_zeros(df):\n",
    "    df = df.drop(df[df['x'] == 0].index)\n",
    "    df = df.drop(df[df['y'] == 0].index)\n",
    "    df = df.drop(df[df['z'] == 0].index)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "# Function to remove outliers\n",
    "def remove_outliers(df):\n",
    "    df = df[(df['x'] < 30)]\n",
    "    df = df[(df['y'] < 30)]\n",
    "    df = df[(df['z'] < 7.5) & (df['z'] > 2)]\n",
    "    df = df[(df['table'] < 80) & (df['table'] > 40)]\n",
    "    df = df[(df['depth'] < 75) & (df['depth'] > 45)]\n",
    "    return df\n",
    "\"\"\"\n",
    "\n",
    "def remove_outliers(df):\n",
    "    if 'x' in df.columns:\n",
    "        df = df[df['x'] < 30]\n",
    "    if 'y' in df.columns:\n",
    "        df = df[df['y'] < 30]\n",
    "    if 'z' in df.columns:\n",
    "        df = df[(df['z'] < 7.5) & (df['z'] > 2)]\n",
    "    if 'table' in df.columns:\n",
    "        df = df[(df['table'] < 80) & (df['table'] > 40)]\n",
    "    if 'depth' in df.columns:\n",
    "        df = df[(df['depth'] < 75) & (df['depth'] > 45)]\n",
    "    return df\n",
    "\n",
    "# Function to remove duplicates\n",
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "# Function to impute values:\n",
    "def imputation(df):\n",
    "    # Calculate the median of each column\n",
    "    median_x = df.loc[df['x'] != 0, 'x'].median()\n",
    "    median_y = df.loc[df['y'] != 0, 'y'].median()\n",
    "    median_z = df.loc[df['z'] != 0, 'z'].median()\n",
    "\n",
    "    # Replace values equal to 0 by the corresponding median.\n",
    "    df['x'] = df['x'].replace(0, median_x)\n",
    "    df['y'] = df['y'].replace(0, median_y)\n",
    "    df['z'] = df['z'].replace(0, median_z)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20371a44",
   "metadata": {},
   "source": [
    "### Functions to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14ac819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df):\n",
    "    df_enc = df.copy()\n",
    "\n",
    "    # Obtain the dataframe encoded\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            enc_label = LabelEncoder()\n",
    "            df_enc[column] = enc_label.fit_transform(df[column])\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843efd9b",
   "metadata": {},
   "source": [
    "### Functions to features ingeniering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c28e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ing(df_features):\n",
    "    #print('Dataframe features: ',df_features.head())\n",
    "    # Test the depth calculate\n",
    "    df_features['depth_mm'] = (df_features['z']*2)/(df_features['x'] + df_features['y'])\n",
    "    # Obtain the average girdle diameter\n",
    "    df_features['avg_girdle'] = (df_features['z'])/(df_features['depth_mm'])\n",
    "    # Obtain table in mm\n",
    "    df_features['table_mm'] = (df_features['avg_girdle'])*(df_features['table'])/100\n",
    "    # Obtain table*depth\n",
    "    df_features['table_depth'] = (df_features['table'])/(df_features['depth'])\n",
    "    # Obtain x, y, z\n",
    "    df_features['xyz'] = (df_features['x'])*(df_features['y'])*(df_features['z'])\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e60b66",
   "metadata": {},
   "source": [
    "### Functions to remove uncorrelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a134c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete features without correlation with price (train data)\n",
    "def delete_features_train(df):\n",
    "    # Calculate correlation matrix, round with two decimmals\n",
    "    corr_matrix = round(df.corr(numeric_only=True).abs(),2)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    #sns.set (rc = {'figure.figsize':(16, 6)})\n",
    "    #sns.heatmap(corr_matrix, center=0, cmap='BrBG', annot=True)\n",
    "\n",
    "    # Find features with correlation greater than 0.90\n",
    "    to_drop = corr_matrix.columns[corr_matrix['price'] <= 0.1]\n",
    "    #print(to_drop)\n",
    "\n",
    "    # Drop features\n",
    "    df_correct = df\n",
    "    df_correct.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df_correct,to_drop\n",
    "\n",
    "# Function to delete features without correlation with price (test data)\n",
    "def delete_features_test(df, to_drop):\n",
    "    # Calculate correlation matrix, round with two decimmals\n",
    "    corr_matrix = round(df.corr(numeric_only=True).abs(),2)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    #sns.set (rc = {'figure.figsize':(16, 6)})\n",
    "    #sns.heatmap(corr_matrix, center=0, cmap='BrBG', annot=True)\n",
    "\n",
    "    # Drop features\n",
    "    df_correct = df\n",
    "    df_correct.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2cde2",
   "metadata": {},
   "source": [
    "### Functions to scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ca07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling function\n",
    "def stardard_scale_train(df):\n",
    "    # Split features and target\n",
    "    X = df.drop(columns=['price'])  # Features\n",
    "    y = df['price']  # Target\n",
    "    columns = X.columns\n",
    "    # Scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale X\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Scale y. With reshape function, we are converting y_v1 into a single column matrix, i.e., one column and as many rows \n",
    "    # as it originally had elements. With flatten, this flattens the resulting array.\n",
    "    y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "    # Return\n",
    "    return X_scaled,y_scaled,columns\n",
    "\n",
    "# Scaling function\n",
    "def stardard_scale_test(df):\n",
    "    X = df\n",
    "    columns = X.columns\n",
    "    # Scaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale X\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled,columns\n",
    "\n",
    "# Invert the scaling\n",
    "def inverse_scaler(df):\n",
    "    return scal.inverse_transform(df.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3e047",
   "metadata": {},
   "source": [
    "# Imports\n",
    "### Train data\n",
    "The query use this terminology:\n",
    "- pro = diamonds_properties \n",
    "- cut = diamonds_cut \n",
    "- col = diamonds_color\n",
    "- cla = diamonds_clarity\n",
    "- tra = diamonds_transactional\n",
    "- cit = diamonds_city\n",
    "- dim = diamonds_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2342a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premium</td>\n",
       "      <td>J</td>\n",
       "      <td>VS2</td>\n",
       "      <td>4268</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>1.21</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS2</td>\n",
       "      <td>505</td>\n",
       "      <td>Kimberly</td>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fair</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>2686</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>0.71</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>738</td>\n",
       "      <td>Kimberly</td>\n",
       "      <td>0.41</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>4882</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>1.02</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cut color clarity  price       city  carat  depth  table     x     y  \\\n",
       "0    Premium     J     VS2   4268      Dubai   1.21   62.4   58.0  6.83  6.79   \n",
       "1  Very Good     H     VS2    505   Kimberly   0.32   63.0   57.0  4.35  4.38   \n",
       "2       Fair     G     VS1   2686  Las Vegas   0.71   65.5   55.0  5.62  5.53   \n",
       "3       Good     D     SI1    738   Kimberly   0.41   63.8   56.0  4.68  4.72   \n",
       "4      Ideal     G     SI1   4882      Dubai   1.02   60.5   59.0  6.55  6.51   \n",
       "\n",
       "      z  \n",
       "0  4.25  \n",
       "1  2.75  \n",
       "2  3.65  \n",
       "3  3.00  \n",
       "4  3.95  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a connection to a file called 'file.db'\n",
    "con = duckdb.connect(\"../data/train/diamonds_train.db\")\n",
    "\n",
    "# Query to extract data from database\n",
    "query_full = \"\"\"\n",
    "SELECT\n",
    "    --tra.index_id,\n",
    "    cut.cut,\n",
    "    col.color,\n",
    "    cla.clarity,\n",
    "    tra.price,\n",
    "    cit.city,\n",
    "    tra.carat,\n",
    "    dim.depth,\n",
    "    dim.table,\n",
    "    dim.x,\n",
    "    dim.y,\n",
    "    dim.z\n",
    "FROM diamonds_properties AS pro\n",
    "JOIN diamonds_cut AS cut ON pro.cut_id = cut.cut_id\n",
    "JOIN diamonds_color AS col ON pro.color_id = col.color_id\n",
    "JOIN diamonds_clarity AS cla ON pro.clarity_id = cla.clarity_id\n",
    "JOIN diamonds_transactional as tra ON pro.index_id = tra.index_id\n",
    "JOIN diamonds_city AS cit ON tra.city_id = cit.city_id\n",
    "JOIN diamonds_dimensions AS dim ON pro.index_id = dim.index_id\n",
    "\"\"\"\n",
    "\n",
    "diamond_train_df = con.execute(query_full).df()\n",
    "diamond_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a461f396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.18</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.57</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.32</td>\n",
       "      <td>4.57</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.90</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  carat        cut color clarity  depth  table     x     y     z  \\\n",
       "0   0   0.79  Very Good     F     SI1   62.7   60.0  5.82  5.89  3.67   \n",
       "1   1   1.20      Ideal     J     VS1   61.0   57.0  6.81  6.89  4.18   \n",
       "2   2   1.57    Premium     H     SI1   62.2   61.0  7.38  7.32  4.57   \n",
       "3   3   0.90  Very Good     F     SI1   63.8   54.0  6.09  6.13  3.90   \n",
       "4   4   0.50  Very Good     F     VS1   62.9   58.0  5.05  5.09  3.19   \n",
       "\n",
       "        city  \n",
       "0  Amsterdam  \n",
       "1      Surat  \n",
       "2   Kimberly  \n",
       "3   Kimberly  \n",
       "4  Amsterdam  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamond_test_df = pd.read_csv(\"../data/test/diamonds_test.csv\")\n",
    "diamond_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68574f74",
   "metadata": {},
   "source": [
    "# Transformation: version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056ec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version1(df, data_type, to_drop=[]):\n",
    "    # Transformations\n",
    "    df = drop_zeros(df)   # Drop zeros\n",
    "    df = remove_outliers(df)   # Remove outliers\n",
    "    df = remove_duplicates(df)   # Remove duplicates\n",
    "    df = encoder(df)   # Encoding\n",
    "    df = feature_ing(df)   # Feature ingeniering\n",
    "    \n",
    "    # In case of test data, the function needs drops_columns to remonve the same data\n",
    "    if data_type == 'train':\n",
    "        # Drop features\n",
    "        df, to_drop = delete_features_train(df)\n",
    "        # Scale   \n",
    "        X_scaled, y_scaled, columns = stardard_scale_train(df)\n",
    "        # Numpy array to dataframe\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=columns)\n",
    "        y_scaled_df = pd.DataFrame({'price': y_scaled})\n",
    "        # return\n",
    "        return X_scaled_df, y_scaled_df, to_drop\n",
    "    \n",
    "    elif data_type == 'test':\n",
    "        # Drop features\n",
    "        df = delete_features_test(df, to_drop)\n",
    "        # Scale\n",
    "        X_scaled, columns = stardard_scale_test(df)\n",
    "        # Numpy array to dataframe\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=columns)\n",
    "        # return\n",
    "        return X_scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692502bd",
   "metadata": {},
   "source": [
    "# Transformation: version 1 (without scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b89e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version1_without_scaler(df, data_type, to_drop=[]):\n",
    "    # Transformations\n",
    "    df = drop_zeros(df)   # Drop zeros\n",
    "    df = remove_outliers(df)   # Remove outliers\n",
    "    df = remove_duplicates(df)   # Remove duplicates\n",
    "    df = encoder(df)   # Encoding\n",
    "    df = feature_ing(df)   # Feature ingeniering\n",
    "    \n",
    "    # In case of test data, the function needs drops_columns to remonve the same data\n",
    "    if data_type == 'train':\n",
    "        # Drop features\n",
    "        df, to_drop = delete_features_train(df)\n",
    "        # return\n",
    "        return df, to_drop\n",
    "    \n",
    "    elif data_type == 'test':\n",
    "        # Drop features\n",
    "        df = delete_features_test(df, to_drop)\n",
    "        # return\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd1463",
   "metadata": {},
   "source": [
    "# Transformation: version 2 (without scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de7c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version1_without_scaler(df, \n",
    "                            drop_zeros_var=0, \n",
    "                            imputation_var=0, \n",
    "                            remove_outliers_var=0, \n",
    "                            remove_duplicates_var=0,\n",
    "                            encoder_var=1,\n",
    "                            feature_ing_var=0,\n",
    "                            delete_features_var=0):\n",
    "    # Transformations\n",
    "    if drop_zeros_var == 1:\n",
    "        df = drop_zeros(df)   # Drop zeros\n",
    "    if imputation_var ==1:\n",
    "        df = imputation(df)\n",
    "    if remove_outliers_var == 1:\n",
    "        df = remove_outliers(df)   # Remove outliers\n",
    "    if remove_duplicates_var == 1:\n",
    "        df = remove_duplicates(df)   # Remove duplicates\n",
    "    if encoder_var == 1:\n",
    "        df = encoder(df)   # Encoding\n",
    "    if feature_ing_var == 1:\n",
    "        df = feature_ing(df)   # Feature ingeniering\n",
    "    if delete_features_var == 1:\n",
    "        df, to_drop = delete_features_train(df) # Drop features\n",
    "        print(to_drop)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform train data\n",
    "df_transform = version1_without_scaler(diamond_train_df)\n",
    "print('Train dataframe shape: ', diamond_train_df.shape, ' | X train shpape: ', df_transform.shape, ' | Difference: ', \n",
    "     diamond_train_df.shape[0]-df_transform.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff04670",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7184006",
   "metadata": {},
   "source": [
    "### Use only the complete dataset where I know the price column (with scaling in this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd91a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Pipelines\n",
    "\n",
    "#Linear Regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler_1',StandardScaler()),\n",
    "    ('lr_classifier',LinearRegression())\n",
    "])\n",
    "# knn\n",
    "knn_pipline =Pipeline([\n",
    "    ('scaler_2' ,StandardScaler()),\n",
    "    ('knn_classifier',KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "#XGB\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler_3', StandardScaler()),\n",
    "    ('xgb_classifier', XGBRegressor())\n",
    "])\n",
    "\n",
    "\n",
    "#Decision Tree\n",
    "dt_pipeline = Pipeline([\n",
    "    ('scaler_4', StandardScaler()),\n",
    "    ('dt_classifier', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "#Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler_5', StandardScaler()),\n",
    "    ('rf_classifier', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "#pipelines = [lr_pipeline,knn_pipline,dt_pipeline,rf_pipeline]   #,xgb_pipeline\n",
    "#models = ['Linear Regression', 'KNN', 'Decision Tree', 'Random Forest']   #, 'XGB'\n",
    "\n",
    "pipelines = [rf_pipeline, xgb_pipeline]\n",
    "models = ['Random Forest', 'XGB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28435a9b",
   "metadata": {},
   "source": [
    "# Automate the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639dcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version1_without_scaler2(df, drop_zeros_var, imputation_var, remove_outliers_var, remove_duplicates_var,\n",
    "                             feature_ing_var, delete_features_var, encoder_var=1):\n",
    "    # Transformations\n",
    "    if drop_zeros_var == 1:\n",
    "        df = drop_zeros(df)   # Drop zeros\n",
    "        \n",
    "    if imputation_var ==1:\n",
    "        df = imputation(df)   # Imputation data\n",
    "        \n",
    "    if remove_outliers_var == 1:\n",
    "        df = remove_outliers(df)   # Remove outliers\n",
    "        \n",
    "    if remove_duplicates_var == 1:\n",
    "        df = remove_duplicates(df)   # Remove duplicates\n",
    "        \n",
    "    if encoder_var == 1:\n",
    "        df = encoder(df)   # Encoding\n",
    "        \n",
    "    if feature_ing_var == 1:\n",
    "        df = feature_ing(df)   # Feature ingeniering\n",
    "        \n",
    "    if delete_features_var == 1:\n",
    "        df, to_drop = delete_features_train(df)   # Drop features\n",
    "        #print(to_drop)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05656bbe",
   "metadata": {},
   "source": [
    "### Diferent processing combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c7c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores para las variables excepto df\n",
    "drop_zeros_var_vals       = [0, 1]   # [0, 1]\n",
    "imputation_var_vals       = [0, 1]\n",
    "remove_outliers_var_vals  = [0, 1]\n",
    "remove_duplicates_var_vals= [0, 1]\n",
    "feature_ing_var_vals      = [0]\n",
    "delete_features_var_vals  = [0]\n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "combinations = itertools.product(drop_zeros_var_vals,\n",
    "                                  imputation_var_vals,\n",
    "                                  remove_outliers_var_vals,\n",
    "                                  remove_duplicates_var_vals,\n",
    "                                  feature_ing_var_vals,\n",
    "                                  delete_features_var_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30824545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_process_choice(df, combinations, pipelines, models):\n",
    "    results = []\n",
    "    for comb in combinations:\n",
    "        # Combinations\n",
    "        drop_zeros_var, imputation_var, remove_outliers_var, remove_duplicates_var, feature_ing_var, delete_features_var = comb\n",
    "        \n",
    "        # Transform train data\n",
    "        df_transform = version1_without_scaler2(df,\n",
    "                                                drop_zeros_var,\n",
    "                                                imputation_var,\n",
    "                                                remove_outliers_var,\n",
    "                                                remove_duplicates_var,\n",
    "                                                feature_ing_var,\n",
    "                                                delete_features_var)\n",
    "\n",
    "        # splitting the dataset in test and train data . The prece will be the Target and the other columns the features\n",
    "        X = df_transform.drop('price',axis = 1)\n",
    "        y = df_transform['price']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "        \n",
    "        # Fit our models to the training data\n",
    "        for i in pipelines :\n",
    "            i.fit(X_train , y_train)\n",
    "        \n",
    "        encoder_var = 1\n",
    "        cv_results = []\n",
    "        for i, model in enumerate(pipelines):\n",
    "            # Cross validation\n",
    "            cv_score = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "            cv_results.append(cv_score)\n",
    "\n",
    "            # Test the result\n",
    "            pred = model.predict(X_test)\n",
    "            \n",
    "            # Store the results in the results list\n",
    "            results.append({'model': models[i],\n",
    "                            'cv_score': cv_score.mean(),\n",
    "                            'prediction': np.sqrt(metrics.mean_squared_error(y_test, pred)),\n",
    "                            'drop_zeros_var': drop_zeros_var, \n",
    "                            'imputation_var': imputation_var, \n",
    "                            'remove_outliers_var': remove_outliers_var, \n",
    "                            'remove_duplicates_var': remove_duplicates_var, \n",
    "                            'encoder_var': encoder_var, \n",
    "                            'feature_ing_var': feature_ing_var, \n",
    "                            'delete_features_var': delete_features_var})\n",
    "            \n",
    "            print(comb, model, np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36890a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Parameters\n",
    "pipelines = [rf_pipeline]   #[lr_pipeline, knn_pipline, dt_pipeline, rf_pipeline, xgb_pipeline]\n",
    "models = ['Random Forest']   #['Linear Regression', 'KNN', 'Decision Tree', 'Random Forest', 'XGB']\n",
    "\n",
    "# Execute functions and sort data\n",
    "result_df = automate_process_choice(diamond_train_df, combinations, pipelines, models)\n",
    "result_df_sorted = result_df.sort_values(by='prediction')\n",
    "result_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d718d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result1 = result_df_sorted.head(20)\n",
    "prediction_result1.to_csv('prediction_12-02-24')\n",
    "#prediction_result1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abaaf9c",
   "metadata": {},
   "source": [
    "### Diferent features combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d734dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_features_choice(df, pipelines, models):\n",
    "    # Features options\n",
    "    cut         = [0]\n",
    "    color       = [0]   # [0, 1]\n",
    "    clarity     = [0]\n",
    "    city        = [0]\n",
    "    carat       = [1]\n",
    "    depth       = [0]\n",
    "    table       = [1]\n",
    "    x           = [1]\n",
    "    y           = [1]\n",
    "    z           = [1]\n",
    "    depth_mm    = [1]\n",
    "    avg_girdle  = [1]\n",
    "    table_mm    = [1]\n",
    "    table_depth = [1]\n",
    "    xyz         = [0]\n",
    "    \n",
    "\n",
    "    # Generar todas las combinaciones posibles\n",
    "    combinations_features = itertools.product(cut, color, clarity, city, carat, depth, table, x, y, z, depth_mm, \n",
    "                                              avg_girdle, table_mm, table_depth, xyz)\n",
    "    \n",
    "    results = []\n",
    "    for comb in combinations_features:\n",
    "        # Transform train data\n",
    "        df_transform = version1_without_scaler2(df,\n",
    "                                                drop_zeros_var=0,\n",
    "                                                imputation_var=0,\n",
    "                                                remove_outliers_var=1,\n",
    "                                                remove_duplicates_var=1,\n",
    "                                                feature_ing_var=1,\n",
    "                                                delete_features_var=0)\n",
    "        \n",
    "        # Obtain the features to train the model\n",
    "        cut, color, clarity, city, carat, depth, table, x, y, z, depth_mm,\\\n",
    "            avg_girdle, table_mm, table_depth, xyz = comb\n",
    "        \n",
    "        variables_con_valor_1 = []\n",
    "        if cut == 1:\n",
    "            variables_con_valor_1.append('cut')\n",
    "        if color == 1:\n",
    "            variables_con_valor_1.append('color')\n",
    "        if clarity == 1:\n",
    "            variables_con_valor_1.append('clarity')\n",
    "        if city == 1:\n",
    "            variables_con_valor_1.append('city')\n",
    "        if carat == 1:\n",
    "            variables_con_valor_1.append('carat')\n",
    "        if depth == 1:\n",
    "            variables_con_valor_1.append('depth')\n",
    "        if table == 1:\n",
    "            variables_con_valor_1.append('table')\n",
    "        if x == 1:\n",
    "            variables_con_valor_1.append('x')\n",
    "        if y == 1:\n",
    "            variables_con_valor_1.append('y')\n",
    "        if z == 1:\n",
    "            variables_con_valor_1.append('z')\n",
    "        if depth_mm == 1:\n",
    "            variables_con_valor_1.append('depth_mm')\n",
    "        if avg_girdle == 1:\n",
    "            variables_con_valor_1.append('avg_girdle')\n",
    "        if table_mm == 1:\n",
    "            variables_con_valor_1.append('table_mm')\n",
    "        if table_depth == 1:\n",
    "            variables_con_valor_1.append('table_depth')\n",
    "        if xyz == 1:\n",
    "            variables_con_valor_1.append('xyz')\n",
    "            \n",
    "        # Obtain the price column\n",
    "        price = df_transform['price']\n",
    "        \n",
    "        # Select the features\n",
    "        selected_f_df = df_transform[variables_con_valor_1]\n",
    "        \n",
    "        # Fix the features dataframe and price colunm\n",
    "        new_df = pd.concat([selected_f_df, price], axis=1)\n",
    "\n",
    "        # splitting the dataset in test and train data . The prece will be the Target and the other columns the features\n",
    "        X = new_df.drop('price',axis = 1)\n",
    "        y = new_df['price']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "        \n",
    "        # There are combinations in which all values are zero, in that case it isn't necessary train model\n",
    "        if new_df.shape[1] > 1:\n",
    "            # Fit our models to the training data\n",
    "            for i in pipelines :\n",
    "                i.fit(X_train , y_train)\n",
    "\n",
    "            cv_results = []\n",
    "            for i, model in enumerate(pipelines):\n",
    "                # Cross validation\n",
    "                cv_score = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "                cv_results.append(cv_score)\n",
    "\n",
    "                # Test the result\n",
    "                pred = model.predict(X_test)\n",
    "\n",
    "                # Store the results in the results list\n",
    "                results.append({'model': models[i],\n",
    "                                'cv_score': cv_score.mean(),\n",
    "                                'prediction': np.sqrt(metrics.mean_squared_error(y_test, pred)),\n",
    "                                'drop_zeros_var': 0, \n",
    "                                'imputation_var': 0, \n",
    "                                'remove_outliers_var': 1, \n",
    "                                'remove_duplicates_var': 1, \n",
    "                                'encoder_var': 1, \n",
    "                                'feature_ing_var': 1,   # This is necessary to test which feature combination is the best\n",
    "                                'delete_features_var': 0,\n",
    "                                'color':color,\n",
    "                                'carat':carat,\n",
    "                                'table':table,\n",
    "                                'x':x,\n",
    "                                'y':y,\n",
    "                                'z':z,\n",
    "                                'depth_mm':depth_mm,\n",
    "                                'avg_girdle':avg_girdle,\n",
    "                                'table_mm':table_mm,\n",
    "                                'table_depth':table_depth,\n",
    "                                'xyz':xyz})\n",
    "                print(comb, model, np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0781024e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0) Pipeline(steps=[('scaler_5', StandardScaler()),\n",
      "                ('rf_classifier', RandomForestRegressor())]) 1360.6705881365433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>drop_zeros_var</th>\n",
       "      <th>imputation_var</th>\n",
       "      <th>remove_outliers_var</th>\n",
       "      <th>remove_duplicates_var</th>\n",
       "      <th>encoder_var</th>\n",
       "      <th>feature_ing_var</th>\n",
       "      <th>delete_features_var</th>\n",
       "      <th>...</th>\n",
       "      <th>carat</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>depth_mm</th>\n",
       "      <th>avg_girdle</th>\n",
       "      <th>table_mm</th>\n",
       "      <th>table_depth</th>\n",
       "      <th>xyz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-1381.38826</td>\n",
       "      <td>1360.670588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0        4268\n",
       "1         505\n",
       "2        2686\n",
       "3   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           model    cv_score   prediction  drop_zeros_var  imputation_var  \\\n",
       "0  Random Forest -1381.38826  1360.670588               0               0   \n",
       "\n",
       "   remove_outliers_var  remove_duplicates_var  encoder_var  feature_ing_var  \\\n",
       "0                    1                      1            1                1   \n",
       "\n",
       "   delete_features_var  ...  carat  table  x  \\\n",
       "0                    0  ...      1      1  1   \n",
       "\n",
       "                                                   y  z  depth_mm  avg_girdle  \\\n",
       "0  0        4268\n",
       "1         505\n",
       "2        2686\n",
       "3   ...  1         1           1   \n",
       "\n",
       "   table_mm  table_depth  xyz  \n",
       "0         1            1    0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "pipelines = [rf_pipeline]   #[lr_pipeline, knn_pipline, dt_pipeline, rf_pipeline, xgb_pipeline]\n",
    "models = ['Random Forest']   #['Linear Regression', 'KNN', 'Decision Tree', 'Random Forest', 'XGB']\n",
    "\n",
    "# Execute functions and sort data\n",
    "result_df_features = automate_features_choice(diamond_train_df, pipelines, models)\n",
    "result_df_features_sorted = result_df_features.sort_values(by='prediction')\n",
    "result_df_features_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c2555e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>cv_score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>drop_zeros_var</th>\n",
       "      <th>imputation_var</th>\n",
       "      <th>remove_outliers_var</th>\n",
       "      <th>remove_duplicates_var</th>\n",
       "      <th>encoder_var</th>\n",
       "      <th>feature_ing_var</th>\n",
       "      <th>delete_features_var</th>\n",
       "      <th>color</th>\n",
       "      <th>carat</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>depth_mm</th>\n",
       "      <th>avg_girdle</th>\n",
       "      <th>table_mm</th>\n",
       "      <th>table_depth</th>\n",
       "      <th>xyz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-1381.38826</td>\n",
       "      <td>1360.670588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0        4268\n",
       "1         505\n",
       "2        2686\n",
       "3   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model    cv_score   prediction  drop_zeros_var  imputation_var  \\\n",
       "0  Random Forest -1381.38826  1360.670588               0               0   \n",
       "\n",
       "   remove_outliers_var  remove_duplicates_var  encoder_var  feature_ing_var  \\\n",
       "0                    1                      1            1                1   \n",
       "\n",
       "   delete_features_var  color  carat  table  x  \\\n",
       "0                    0      0      1      1  1   \n",
       "\n",
       "                                                   y  z  depth_mm  avg_girdle  \\\n",
       "0  0        4268\n",
       "1         505\n",
       "2        2686\n",
       "3   ...  1         1           1   \n",
       "\n",
       "   table_mm  table_depth  xyz  \n",
       "0         1            1    0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "result_df_features_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55531ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "cut         = [1]\n",
    "color       = [1]   # [0, 1]\n",
    "clarity     = [1]\n",
    "city        = [1]\n",
    "carat       = [1]\n",
    "depth       = [1]\n",
    "table       = [1]\n",
    "x           = [1]\n",
    "y           = [1]\n",
    "z           = [1]\n",
    "depth_mm    = [0, 1]\n",
    "avg_girdle  = [0, 1]\n",
    "table_mm    = [0, 1]\n",
    "table_depth = [0, 1]\n",
    "xyz         = [0, 1]\n",
    "    \n",
    "\n",
    "# Generar todas las combinaciones posibles\n",
    "combinations_features = itertools.product(cut, color, clarity, city, carat, depth, table, x, y, z, depth_mm, \n",
    "                                          avg_girdle, table_mm, table_depth, xyz)\n",
    "features_list = ['cut', 'color', 'clarity', 'city', 'carat', 'depth', 'table', 'x', 'y', 'z', 'depth_mm', 'avg_girdle', \n",
    "                 'table_mm', 'table_depth', 'xyz']\n",
    "\n",
    "for comb in combinations_features:\n",
    "    print(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe651fa4",
   "metadata": {},
   "source": [
    "# Optimaze the model\n",
    "### Obtain the data with the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform = version1_without_scaler2(diamond_train_df,\n",
    "                                        drop_zeros_var=1,\n",
    "                                        imputation_var=0,\n",
    "                                        remove_outliers_var=1,\n",
    "                                        remove_duplicates_var=1,\n",
    "                                        feature_ing_var=0,\n",
    "                                        delete_features_var=0)\n",
    "\n",
    "# splitting the dataset in test and train data . The prece will be the Target and the other columns the features\n",
    "X = df_transform.drop('price',axis = 1)\n",
    "y = df_transform['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671579b",
   "metadata": {},
   "source": [
    "### Obtain the best parameters of the model\n",
    "#### Gradient busting regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b99c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 250, 500, 750, 1000],  # Number of boosting stages to be run.\n",
    "              'learning_rate': [0.01, 0.05, 0.1],  # Rate at which the contribution of each tree is shrunk.\n",
    "              'max_depth': [None, 3, 6, 10],  # Maximum depth of the individual regression estimators.\n",
    "              'min_samples_split': [2, 10],  # Minimum number of samples required to split an internal node.\n",
    "              'min_samples_leaf': [1, 4],  # Minimum number of samples required to be at a leaf node.\n",
    "              'max_features': [None, 'sqrt', 'log2']  # The number of features to consider when looking for the best split.\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fe8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "param_grid = {'n_estimators': [400, 600],  # Number of boosting stages to be run.\n",
    "              'learning_rate': [0.01],  # Rate at which the contribution of each tree is shrunk.\n",
    "              'max_depth': [None],  # Maximum depth of the individual regression estimators.\n",
    "              'min_samples_split': [2],  # Minimum number of samples required to split an internal node.\n",
    "              'min_samples_leaf': [4, 6],  # Minimum number of samples required to be at a leaf node.\n",
    "              'max_features': [None, 'sqrt', 'log2']  # The number of features to consider when looking for the best split.\n",
    "              }\n",
    "\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=3,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1088be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Obtain the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Best hyperparameters: ', grid_search.best_params_, '\\n')\n",
    "print('Best score: ', -grid_search.best_score_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447181d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "param_grid = {'n_estimators': [100, 200, 300],  # Number of trees in the forest.\n",
    "              'max_depth': [None, 3, 10],  # Maximum depth of the trees.\n",
    "              'min_samples_split': [2, 10],  # Minimum number of samples required to split an internal node.\n",
    "              'min_samples_leaf': [1, 4],  # Minimum number of samples required to be at a leaf node.\n",
    "              'max_features': [None, 'sqrt', 'log2']  # Number of features to consider when looking for the best split.\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97914c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Obtain the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Best hyperparameters: ', grid_search.best_params_, '\\n')\n",
    "print('Best score: ', -grid_search.best_score_, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40afba1",
   "metadata": {},
   "source": [
    "# Final process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "drop_zeros_var=1,\n",
    "imputation_var=0,\n",
    "remove_outliers_var=1,\n",
    "remove_duplicates_var=1,\n",
    "feature_ing_var=0,\n",
    "delete_features_var=0\n",
    "encoder_var = 1\n",
    "\n",
    "df_transform = version1_without_scaler2(diamond_train_df,\n",
    "                                        drop_zeros_var,\n",
    "                                        imputation_var,\n",
    "                                        remove_outliers_var,\n",
    "                                        remove_duplicates_var,\n",
    "                                        feature_ing_var,\n",
    "                                        delete_features_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34551984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Pipeline\n",
    "pipelines = [rf_pipeline]\n",
    "models = ['Random Forest']\n",
    "\n",
    "# splitting the dataset in test and train data . The prece will be the Target and the other columns the features\n",
    "X = df_transform.drop('price',axis = 1)\n",
    "y = df_transform['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "#print(X_train.head())\n",
    "        \n",
    "# Fit our models to the training data\n",
    "for i in pipelines :\n",
    "    i.fit(X_train , y_train)\n",
    "#print(X_train.head())\n",
    "        \n",
    "cv_results = []\n",
    "for i, model in enumerate(pipelines):\n",
    "    # Cross validation\n",
    "    cv_score = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "    cv_results.append(cv_score)\n",
    "    #print(\"%s: %f \" % (models[i], cv_score.mean()))\n",
    "\n",
    "    # Test the result\n",
    "    pred = model.predict(X_test)\n",
    "    #print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "\n",
    "    # Store the results in the results list\n",
    "    results.append({'drop_zeros_var': drop_zeros_var, \n",
    "                    'imputation_var': imputation_var, \n",
    "                    'remove_outliers_var': remove_outliers_var, \n",
    "                    'remove_duplicates_var': remove_duplicates_var, \n",
    "                    'encoder_var': encoder_var, \n",
    "                    'feature_ing_var': feature_ing_var, \n",
    "                    'delete_features_var': delete_features_var,\n",
    "                    f'{models[i]}': cv_score.mean(),\n",
    "                    'prediction': np.sqrt(metrics.mean_squared_error(y_test, pred))})\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eda18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e42f1",
   "metadata": {},
   "source": [
    "# Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4506028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "names = [\"Linear Regression\", \"Ridge Regression\", \"Lasso Regression\",\n",
    "         \"Decision Tree Regressor\", \"Random Forest Regressor\", \"Gradient Boosting Regressor\",\n",
    "         \"Adaboost Regressor\", \"BaggingRegressor\", \"ExtraTreesRegressor\",\"XGBRegressor\", \"XGBRFRegressor\"]\n",
    "models = [LinearRegression(), Ridge(), Lasso(), DecisionTreeRegressor(),\n",
    "          RandomForestRegressor(), GradientBoostingRegressor(), \n",
    "          AdaBoostRegressor(), BaggingRegressor(), ExtraTreesRegressor(),XGBRegressor(), XGBRFRegressor()]\n",
    "          \n",
    "names = [\"ExtraTreesRegressor\", \"Random Forest Regressor\"]\n",
    "models = [ExtraTreesRegressor(), RandomForestRegressor()]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3_env",
   "language": "python",
   "name": "m3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
